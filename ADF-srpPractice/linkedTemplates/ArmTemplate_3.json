{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "ADF-srpPractice"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/DF_ALTER_ROW_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_EMPLOYEES",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_AzurePostgreSqlTable1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EmpId as string,",
						"          EmpName as string,",
						"          Gender as string,",
						"          Salary as string,",
						"          Department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 alterRow(deleteIf(equals(Department,'Payroll')),",
						"     updateIf(equals(Department,'HR'))) ~> alterRow1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          gender as string,",
						"          salary as integer,",
						"          department as string",
						"     ),",
						"     deletable:true,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:true,",
						"     keys:['empid'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid = EmpId,",
						"          empname = EmpName,",
						"          gender = Gender,",
						"          salary = Salary,",
						"          department = Department",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_ASSERTION_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_EMPLOYEES1",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "DS_SOURCE_DEPARTMENTS1",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_EMPLOYEES1_GOOD_DATA",
								"type": "DatasetReference"
							},
							"name": "sinkGoodData"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_EMPLOYEES1_BAD_DATA",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "assert1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "filterGoodrows"
						},
						{
							"name": "filterBadRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          name as string,",
						"          gender as string,",
						"          doj as string,",
						"          depid as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          deptId as string,",
						"          deptName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source2",
						"source1, source2 assert(expectTrue(!isNull(toDate(doj,'yyyyMMdd')), false, 'assertDOJ', null, \"Here we are checking DOJ rows\"),",
						"     expectUnique(empid, false, 'assertEMPID', null, \"Unique MEPID\"),",
						"     expectExists(depid == deptId, false, 'assertEXIST')) ~> assert1",
						"assert1 derive(isErrorRow = isError(),",
						"          isIncorrectDeptRow = hasError('assertEXIST')) ~> derivedColumn1",
						"derivedColumn1 filter(isErrorRow == false()) ~> filterGoodrows",
						"derivedColumn1 filter(isErrorRow == true()) ~> filterBadRows",
						"filterGoodrows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employees1.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid,",
						"          name,",
						"          gender,",
						"          doj,",
						"          deptId = depid",
						"     ),",
						"     partitionBy('hash', 1)) ~> sinkGoodData",
						"filterBadRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employees1.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid,",
						"          name,",
						"          gender,",
						"          doj,",
						"          depid",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_CAST_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_EMPLOYEES2",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_GOODROWS_EMPLOYEES2",
								"type": "DatasetReference"
							},
							"name": "sinkGoodData"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_BADROWS_EMPLOYEES2",
								"type": "DatasetReference"
							},
							"name": "sinkBadRows"
						}
					],
					"transformations": [
						{
							"name": "cast1"
						},
						{
							"name": "split1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          name as string,",
						"          gender as string,",
						"          doj as string,",
						"          depid as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 cast(output(",
						"          empid as integer,",
						"          doj as date 'dd/MM/yyyy',",
						"          depid as integer",
						"     ),",
						"     errors: true) ~> cast1",
						"cast1 split(!isError(),",
						"     disjoint: false) ~> split1@(goodRows, badRows)",
						"split1@goodRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employees2_Goodrows.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkGoodData",
						"split1@badRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employees2_badrows.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkBadRows"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_CONDITIONAL_SPLIT')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_JOINED_EMPLOYEES_DEPARTMENTS",
								"type": "DatasetReference"
							},
							"name": "AllHREmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_ItEmployees_SINK",
								"type": "DatasetReference"
							},
							"name": "sinkItEmployeees"
						},
						{
							"dataset": {
								"referenceName": "DS_EXEmployees_SINK",
								"type": "DatasetReference"
							},
							"name": "sinkExEmployees"
						},
						{
							"dataset": {
								"referenceName": "DS_FinanceEmployees_SINK",
								"type": "DatasetReference"
							},
							"name": "sinkfinemployees"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_Purchasing_Employees",
								"type": "DatasetReference"
							},
							"name": "sinkpurchasinEmployees"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_Shipping_Employees",
								"type": "DatasetReference"
							},
							"name": "sinkshippingemployees"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_SalesEmployees",
								"type": "DatasetReference"
							},
							"name": "sinksalesemployees"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_AdministartionEmployees",
								"type": "DatasetReference"
							},
							"name": "sinkadministartionemployees"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_Marketing_Employees",
								"type": "DatasetReference"
							},
							"name": "sinkmarketingemployees"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_Accounting_Employees",
								"type": "DatasetReference"
							},
							"name": "sinkaccountingemployees"
						},
						{
							"dataset": {
								"referenceName": "DS_SINK_Other_Employees",
								"type": "DatasetReference"
							},
							"name": "sinkotheremployees"
						}
					],
					"transformations": [
						{
							"name": "splitbasedondepartments"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string,",
						"          DEPARTMENT_NAME as string,",
						"          LOCATION_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> AllHREmployees",
						"AllHREmployees split(equals(DEPARTMENT_NAME, 'IT'),",
						"     equals(DEPARTMENT_NAME, 'Executive'),",
						"     equals(DEPARTMENT_NAME, 'Finance'),",
						"     equals(DEPARTMENT_NAME, 'Purchasing'),",
						"     equals(DEPARTMENT_NAME, 'Shipping'),",
						"     equals(DEPARTMENT_NAME, 'Sales'),",
						"     equals(DEPARTMENT_NAME, 'Administration'),",
						"     equals(DEPARTMENT_NAME, 'Marketing'),",
						"     equals(DEPARTMENT_NAME, 'Accounting'),",
						"     disjoint: false) ~> splitbasedondepartments@(ITEmployees, ExEmployess, FinanceEmployees, PurchasingEmployees, ShippingEmployees, SalesEmployees, AdministrationEmployees, MarketingEmployees, AccountingEmployees, OtherEmployees)",
						"splitbasedondepartments@ITEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ITEmployees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkItEmployeees",
						"splitbasedondepartments@ExEmployess sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ExEmployees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkExEmployees",
						"splitbasedondepartments@FinanceEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['FinanceEmployees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkfinemployees",
						"splitbasedondepartments@PurchasingEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Purchasing_Emmployees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkpurchasinEmployees",
						"splitbasedondepartments@ShippingEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Shipping_Employees'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkshippingemployees",
						"splitbasedondepartments@SalesEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Sales_Employees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinksalesemployees",
						"splitbasedondepartments@AdministrationEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Administration_Employees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkadministartionemployees",
						"splitbasedondepartments@MarketingEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Marketing_Employees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkmarketingemployees",
						"splitbasedondepartments@AccountingEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Accounting_Employees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkaccountingemployees",
						"splitbasedondepartments@OtherEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Others_Employees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkotheremployees"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_DEPT_FLOWLET_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_DEPARTMENTS_WITH_DUPLICATES",
								"type": "DatasetReference"
							},
							"name": "Department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_OUTPUT",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flowlet1",
							"flowlet": {
								"referenceName": "FLOWLET_REMOVE_DUPLICATES",
								"type": "DataFlowReference",
								"parameters": {}
							}
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Id as string,",
						"          Name as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Department",
						"derivedColumn1 compose(mapColumn(",
						"          id = Id,",
						"          Name,",
						"          gender = Id,",
						"          salary = Id",
						"     ),",
						"     composition: 'FLOWLET_REMOVE_DUPLICATES') ~> flowlet1@(output1)",
						"Department derive(gender = 'abcd',",
						"          salary = 'sharad') ~> derivedColumn1",
						"flowlet1@output1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Departmennts.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          name",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_DERIVED_COLUMNS')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_HR_EMPLOYEES",
								"type": "DatasetReference"
							},
							"name": "HREmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_DERIVED_COLUMNS_HREMPLOYEES_DATA",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> HREmployees",
						"HREmployees derive(FIRST_NAME = upper(FIRST_NAME),",
						"          New_Country_Column = iif(equals(DEPARTMENT_ID,\"100\"), \"India\", \"USA\")) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['HREmployees_Derived_columns.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_EMPLOYEE_MAXSALARY')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_EMPLOYEE",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          name as string,",
						"          gender as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 aggregate(MaxSalary = max(salary)) ~> aggregate1",
						"aggregate1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_EMP_FLOWLETS_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_EMPLOYEE_DUPLICATES",
								"type": "DatasetReference"
							},
							"name": "Employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_OUTPUT",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flowlet1",
							"flowlet": {
								"referenceName": "FLOWLET_REMOVE_DUPLICATES",
								"type": "DataFlowReference",
								"parameters": {}
							}
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          name as string,",
						"          gender as string,",
						"          salary as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"Employee compose(mapColumn(",
						"          id,",
						"          Name = id,",
						"          gender,",
						"          salary",
						"     ),",
						"     composition: 'FLOWLET_REMOVE_DUPLICATES') ~> flowlet1@(output1)",
						"flowlet1@output1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Emp.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          name,",
						"          gender,",
						"          salary",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_FLATTEN_NESTED_JSON')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_SAMPLE_NESTED_JSON",
								"type": "DatasetReference"
							},
							"name": "Samplenested"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_SAMPLE_NESTED_JSON",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          coffee as (region as (id as integer, name as string)[], country as (id as integer, company as string)),",
						"          brewing as (region as (id as integer, name as string)[], country as (id as integer, company as string))",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> Samplenested",
						"Samplenested foldDown(unroll(brewing.region),",
						"     mapColumn(",
						"          Region_id = brewing.region.id,",
						"          region_name = brewing.region.name",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Sample_nested_json.json'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_JOIN_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_HR_EMPLOYEES",
								"type": "DatasetReference"
							},
							"name": "HREmployees"
						},
						{
							"dataset": {
								"referenceName": "DS_SOURCE_HR_DEPARTMENTS",
								"type": "DatasetReference"
							},
							"name": "HRDepartments"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINNK_JOIN_EMPLOYEE_DEPARTMENT",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "HREmployeesjoinonHRDepartments"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> HREmployees",
						"source(output(",
						"          DEPARTMENT_ID as string,",
						"          DEPARTMENT_NAME as string,",
						"          MANAGER_ID as string,",
						"          LOCATION_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> HRDepartments",
						"HREmployees, HRDepartments join(HREmployees@DEPARTMENT_ID == HRDepartments@DEPARTMENT_ID,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> HREmployeesjoinonHRDepartments",
						"HREmployeesjoinonHRDepartments sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     filePattern:'Employee_Department_inner_join',",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          EMPLOYEE_ID,",
						"          FIRST_NAME,",
						"          LAST_NAME,",
						"          EMAIL,",
						"          PHONE_NUMBER,",
						"          HIRE_DATE,",
						"          JOB_ID,",
						"          SALARY,",
						"          COMMISSION_PCT,",
						"          MANAGER_ID = HREmployees@MANAGER_ID,",
						"          DEPARTMENT_ID = HREmployees@DEPARTMENT_ID,",
						"          DEPARTMENT_NAME,",
						"          LOCATION_ID",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_PARSE_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_EmployeeTable_AzurePostgreSqlTable1",
								"type": "DatasetReference"
							},
							"name": "EmployeeTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_PARSE_TRANSFORM_UPDATE",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "parseSkills"
						},
						{
							"name": "parseAddress"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          skills as string,",
						"          address as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> EmployeeTable",
						"EmployeeTable parse(ParseSkills = skills ? (Skill1 as string,",
						"          Skill2 as string,",
						"          Skill3 as string),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseSkills",
						"parseSkills parse(ParseAddress = address ? (city as string,",
						"          Country as string),",
						"     format: 'json',",
						"     documentForm: 'singleDocument') ~> parseAddress",
						"parseAddress sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employee_Parse_Transform.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid,",
						"          empname,",
						"          Skill1 = ParseSkills.Skill1,",
						"          Skill2 = ParseSkills.Skill2,",
						"          Skill3 = ParseSkills.Skill3,",
						"          city = ParseAddress.city,",
						"          Country = ParseAddress.Country",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_RANK_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_HR_EMPLOYEES",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_RANKED_SALARYWISE_DATA",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "rank1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as integer,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 rank(desc(SALARY, true),",
						"     caseInsensitive: true,",
						"     output(Ranking as long),",
						"     dense: true) ~> rank1",
						"rank1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employees_Salarywise_Ranking.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_STRINGIFY_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_DEMO_JSON_FILE",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_STRINGIFY_DEMO_JSON",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "stringify1"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          name as string,",
						"          Skills as string[],",
						"          Contact as (Mobile as string, Landline as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 stringify(ContactStringify = Contact ? string,",
						"     format: 'json') ~> stringify1",
						"stringify1 derive(ContactStringify = toString(ContactStringify)) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Stringify_demo_json.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          name,",
						"          Skills,",
						"          ContactStringify",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_SURROGATE_KEY_TRANSFORMATION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_EMPLOYEE_SURROGATE_KEY",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_SURROGATEKEY_OUTPUT",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "surrogateKey1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          DEPARTMENT_ID as string,",
						"          DEPARTMENT_NAME as string,",
						"          MANAGER_ID as string,",
						"          LOCATION_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees keyGenerate(output(Empkey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"surrogateKey1 select(mapColumn(",
						"          DEPARTMENT_ID,",
						"          DEPARTMENT_NAME,",
						"          MANAGER_ID,",
						"          LOCATION_ID,",
						"          Empkey",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employee_surrogate_key.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_UDF_USERDEFINED')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_EMPLOYEES1_DATA",
								"type": "DatasetReference"
							},
							"name": "Employees1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_DERIVED_COLUMNS_EMPLOYYES1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"udfLibraries": [
						{
							"referenceName": "stringFunction",
							"type": "DataFlowReference"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          name as string,",
						"          gender as string,",
						"          doj as string,",
						"          depid as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees1",
						"Employees1 derive(gender = genderValuesInIntegerRows(gender)) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employees1_Derived_columns.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_VALIDATE_SCHEMA_OPTION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_EMPLOYEES",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_EMPLOYEES",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          EmpId as string,",
						"          EmpName as string,",
						"          Gender as string,",
						"          Salary as string,",
						"          Department as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Employees.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DF_WINDOW_FUNCTION')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_SOURCE_HR_EMPLOYEES",
								"type": "DatasetReference"
							},
							"name": "HREmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SINK_WINDOW_TRANSFORMATION",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "window1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as integer,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> HREmployees",
						"HREmployees window(over(DEPARTMENT_ID),",
						"     asc(SALARY, true),",
						"     Avg_Salary = avg(SALARY),",
						"          Total_Salary = sum(SALARY),",
						"          Dense_Rank = denseRank()) ~> window1",
						"window1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Window_transformation'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/powerquery1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "DS_SOURCE_HR_EMPLOYEES",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> DS_SOURCE_HR_EMPLOYEES",
							"dataset": {
								"referenceName": "DS_SOURCE_HR_EMPLOYEES",
								"type": "DatasetReference"
							}
						}
					],
					"script": "section Section1;\r\nshared DS_SOURCE_HR_EMPLOYEES = let AdfDoc = AzureStorage.DataLakeContents(\"https://srpadfpractice.dfs.core.windows.net/adf-practice/input/Hr_Employees.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared UserQuery = let Source = #\"DS_SOURCE_HR_EMPLOYEES\",\r\n  #\"Removed columns\" = Table.RemoveColumns(Source, {\"EMAIL\"}) in #\"Removed columns\";\r\n",
					"documentLocale": "en-us"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/powerquery2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "DS_SOURCE_HR_EMPLOYEES",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> DS_SOURCE_HR_EMPLOYEES",
							"dataset": {
								"referenceName": "DS_SOURCE_HR_EMPLOYEES",
								"type": "DatasetReference"
							}
						},
						{
							"name": "DS_SOURCE_HR_DEPARTMENTS",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> DS_SOURCE_HR_DEPARTMENTS",
							"dataset": {
								"referenceName": "DS_SOURCE_HR_DEPARTMENTS",
								"type": "DatasetReference"
							}
						}
					],
					"script": "section Section1;\r\nshared DS_SOURCE_HR_EMPLOYEES = let AdfDoc = AzureStorage.DataLakeContents(\"https://srpadfpractice.dfs.core.windows.net/adf-practice/input/Hr_Employees.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared DS_SOURCE_HR_DEPARTMENTS = let AdfDoc = AzureStorage.DataLakeContents(\"https://srpadfpractice.dfs.core.windows.net/adf-practice/input/Hr_Departments.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared UserQuery = let Source = #\"DS_SOURCE_HR_EMPLOYEES\",\r\n  #\"Merged queries\" = Table.NestedJoin(Source, {\"DEPARTMENT_ID\"}, DS_SOURCE_HR_DEPARTMENTS, {\"DEPARTMENT_ID\"}, \"DS_SOURCE_HR_DEPARTMENTS\", JoinKind.Inner),\r\n  #\"Expanded DS_SOURCE_HR_DEPARTMENTS\" = Table.ExpandTableColumn(#\"Merged queries\", \"DS_SOURCE_HR_DEPARTMENTS\", {\"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"}, {\"DEPARTMENT_ID.1\", \"DEPARTMENT_NAME\", \"MANAGER_ID.1\", \"LOCATION_ID\"}),\r\n  #\"Removed columns\" = Table.RemoveColumns(#\"Expanded DS_SOURCE_HR_DEPARTMENTS\", {\"DEPARTMENT_ID.1\"}) in #\"Removed columns\";\r\n",
					"documentLocale": "en-us"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/powerquery3')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "DS_SOURCE_HR_EMPLOYEES",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> DS_SOURCE_HR_EMPLOYEES",
							"dataset": {
								"referenceName": "DS_SOURCE_HR_EMPLOYEES",
								"type": "DatasetReference"
							}
						}
					],
					"script": "section Section1;\r\nshared DS_SOURCE_HR_EMPLOYEES = let AdfDoc = AzureStorage.DataLakeContents(\"https://srpadfpractice.dfs.core.windows.net/adf-practice/input/Hr_Employees.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared UserQuery = let Source = #\"DS_SOURCE_HR_EMPLOYEES\",\r\n  #\"Changed column type\" = Table.TransformColumnTypes(Source, {{\"SALARY\", type number}}),\r\n  #\"Grouped rows\" = Table.Group(#\"Changed column type\", {\"DEPARTMENT_ID\"}, {{\"Total_Salary\", each List.Sum([SALARY]), type nullable number}}) in #\"Grouped rows\";\r\n",
					"documentLocale": "en-us"
				}
			},
			"dependsOn": []
		}
	]
}